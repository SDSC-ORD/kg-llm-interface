{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use-case\n",
    "\n",
    "In this notebook, we showcase a simple question answering task. We will use the [SPHN ontology](https://www.biomedit.ch/rdf/sphn-ontology/sphn), along with a small mock dataset which contains information artificial medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPARQL_TEMPLATE = \"\"\"\n",
    "Generate a SPARQL query to answer the input question. A sample of the knowledge graph schema is provided to help construct the query.\n",
    "After you generate the sparql, you should display it.\n",
    "When generating sparql:\n",
    "* never enclose the sparql in back-quotes.\n",
    "* always include the prefix declarations.\n",
    "* prefer using OPTIONAL when selecting multiple variables.\n",
    "* Allow case-insensitive matching of strings.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question for which you must generate a SPARQL query\n",
    "Information: the schema information in RDF format. This will help you generate the sparql query with the correct format.\n",
    "\n",
    "Question: {question_str}\n",
    "Information:\n",
    "{context_str}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_TEMPLATE = \"\"\"\n",
    "The following describe a user question, associated SPARQL query and the result from executing the query.\n",
    "Based on this information, write an answer in simple terms that describes the results.\n",
    "When appropriate, use markdown formatting to format the results into a table or bullet points.\n",
    "\n",
    "Question:\n",
    "{question_str}\n",
    "Query:\n",
    "{query_str}\n",
    "Result:\n",
    "{result_str}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup a similar configuration as in the nl_sparql notebook, but we have one sparql configuration for the ontology, and one for the instance data, each living in different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aikg.config import ChatConfig, ChromaConfig, SparqlConfig\n",
    "\n",
    "chroma_config = ChromaConfig(\n",
    "    host=\"local\",\n",
    "    port=8000,\n",
    "    collection_name=\"test\",\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    persist_directory=\"/tmp/chroma-test/\",\n",
    ")\n",
    "ontology_config = SparqlConfig(\n",
    "    endpoint=\"../sphn/sphn_ontology_2023_2.ttl\",\n",
    ")\n",
    "kg_config = SparqlConfig(\n",
    "    endpoint=\"../sphn/sphn_mock_data_2023_2.ttl\",\n",
    ")\n",
    "\n",
    "chat_config = ChatConfig(\n",
    "    model_id=\"lmsys/vicuna-7b-v1.3\",\n",
    "    max_new_tokens=48,\n",
    "    max_input_size=2048,\n",
    "    num_output=256,\n",
    "    max_chunk_overlap=20,\n",
    "    answer_template=ANSWER_TEMPLATE,\n",
    "    sparql_template=SPARQL_TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#gYear, Converter=<function parse_date at 0x7f44061e09d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/rdflib/term.py\", line 2084, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-1508+14:00'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#gYear, Converter=<function parse_date at 0x7f44061e09d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/rdflib/term.py\", line 2084, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-2358+01:14'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#gYear, Converter=<function parse_date at 0x7f44061e09d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/rdflib/term.py\", line 2084, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-2221+14:00'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from aikg.utils.llm import setup_llm_chain\n",
    "from aikg.utils.rdf import setup_kg\n",
    "\n",
    "\n",
    "# Use OpenAI API\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# For now, both chains share the same model to spare memory\n",
    "answer_chain = setup_llm_chain(llm, chat_config.answer_template)\n",
    "sparql_chain = setup_llm_chain(llm, chat_config.sparql_template)\n",
    "kg = setup_kg(**kg_config.dict())\n",
    "\n",
    "# Embed ontology\n",
    "from aikg.flows.chroma_build import chroma_build_flow\n",
    "chroma_build_flow(chroma_config, ontology_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmatthey/.cache/pypoetry/virtualenvs/aikg-ULDgE_fB-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from aikg.utils.chroma import setup_client, setup_collection\n",
    "client = setup_client(\n",
    "    chroma_config.host,\n",
    "    chroma_config.port,\n",
    "    chroma_config.persist_directory,\n",
    ")\n",
    "collection = setup_collection(\n",
    "    client,\n",
    "    chroma_config.collection_name,\n",
    "    chroma_config.embedding_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Please give me the number of healthcare encounters recorded per year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREFIX ns1: <http://www.w3.org/2004/02/skos/core#>\n",
      "PREFIX ns2: <https://biomedit.ch/rdf/sphn-ontology/sphn#>\n",
      "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "\n",
      "SELECT (COUNT(*) AS ?encounters) (YEAR(?startDateTime) AS ?year)\n",
      "WHERE {\n",
      "  ?encounter a ns2:HealthcareEncounter ;\n",
      "    ns2:hasStartDateTime ?startDateTime .\n",
      "}\n",
      "GROUP BY ?year\n"
     ]
    }
   ],
   "source": [
    "from aikg.utils.chat import generate_sparql\n",
    "query = generate_sparql(QUESTION, collection, sparql_chain)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(rdflib.term.Literal('2', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')), rdflib.term.Literal('2009', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')))]\n"
     ]
    }
   ],
   "source": [
    "from aikg.utils.rdf import query_kg\n",
    "results = query_kg(kg, query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2009 there were 2 healthcare encounters recorded.\n"
     ]
    }
   ],
   "source": [
    "from aikg.utils.chat import generate_answer\n",
    "print(generate_answer(QUESTION, query, results, answer_chain))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aikg-URVQdnEY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
